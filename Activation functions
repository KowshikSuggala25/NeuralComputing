#4.1 implementation of networks using ReLU, sigmoid and Tanh activation functions.
# Import numpy
import numpy as np

# Input data (AND gate)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([0, 0, 0, 1])

# -------------------------------
# Activation functions
# -------------------------------
def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

# -------------------------------
# Train perceptron-like model
# -------------------------------
def train_with_activation(activation_fn, name):
    np.random.seed(42)
    weights = np.random.randn(2) * 0.01
    bias = np.random.randn() * 0.01
    learning_rate = 0.1
    epochs = 10

    for epoch in range(epochs):
        for i in range(len(X)):
            z = np.dot(X[i], weights) + bias
            a = activation_fn(z)

            # Convert activation to binary prediction
            pred = 1 if a >= 0.5 else 0
            error = y[i] - pred

            # Update weights and bias
            weights += learning_rate * error * X[i]
            bias += learning_rate * error

    print(f"\nActivation Function: {name}")
    print("Trained Weights:", weights)
    print("Trained Bias:", bias)

    for i in range(len(X)):
        z = np.dot(X[i], weights) + bias
        a = activation_fn(z)
        pred = 1 if a >= 0.5 else 0
        print(f"Input: {X[i]} -> Prediction: {pred}")

# -------------------------------
# Try different activations
# -------------------------------
train_with_activation(relu, "ReLU")
train_with_activation(sigmoid, "Sigmoid")
train_with_activation(tanh, "Tanh")


#4.2 Compare performance on simple classification problem
import numpy as np

# Binary classification dataset (AND gate)
X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([0, 0, 0, 1])

# -------------------------------
# Activation functions
# -------------------------------
def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

# -------------------------------
# Evaluate model performance
# -------------------------------
def evaluate(activation_fn, name):
    np.random.seed(42)
    weights = np.random.randn(2) * 0.01
    bias = np.random.randn() * 0.01
    learning_rate = 0.1
    epochs = 10

    # Training loop
    for epoch in range(epochs):
        for i in range(len(X)):
            z = np.dot(X[i], weights) + bias
            a = activation_fn(z)
            pred = 1 if a >= 0.5 else 0
            error = y[i] - pred
            weights += learning_rate * error * X[i]
            bias += learning_rate * error

    # Accuracy calculation
    correct = 0
    print(f"\nActivation Function: {name}")
    for i in range(len(X)):
        z = np.dot(X[i], weights) + bias
        a = activation_fn(z)
        pred = 1 if a >= 0.5 else 0
        print(f"Input: {X[i]} -> Prediction: {pred}")
        if pred == y[i]:
            correct += 1

    accuracy = correct / len(X)
    print(f"Accuracy: {correct}/{len(X)} = {accuracy}")

# -------------------------------
# Compare activations
# -------------------------------
evaluate(relu, "ReLU")
evaluate(sigmoid, "Sigmoid")
evaluate(tanh, "Tanh")


#4.3 Investigate the vanishing Gradient Problem
import numpy as np

X = np.array([[0, 0],
              [0, 1],
              [1, 0],
              [1, 1]])

y = np.array([0, 0, 0, 1])

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def relu(x):
    return np.maximum(0, x)

def d_sigmoid(x):
    s = sigmoid(x)
    return s * (1 - s)

def d_tanh(x):
    return 1 - np.tanh(x)**2

def d_relu(x):
    return 1 if x > 0 else 0

def investigate_gradients(activation_fn, derivative_fn, name):
    np.random.seed(42)
    weights = np.random.randn(2) * 0.01
    bias = np.random.randn() * 0.01
    learning_rate = 0.1
    epochs = 2

    print(f"\nActivation Function: {name}")

    for epoch in range(epochs):
        for i in range(len(X)):
            z = np.dot(X[i], weights) + bias
            a = activation_fn(z)
            pred = 1 if a >= 0.5 else 0
            error = y[i] - pred

            grad = derivative_fn(z)
            print(f"Epoch {epoch}, Input {X[i]}, Gradient: {grad:.6f}")

            weights += learning_rate * error * X[i] * grad
            bias += learning_rate * error * grad

investigate_gradients(sigmoid, d_sigmoid, "Sigmoid")
investigate_gradients(tanh, d_tanh, "Tanh")
investigate_gradients(relu, d_relu, "ReLU")

