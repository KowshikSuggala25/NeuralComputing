# ============================================================
# 9.1 Implement a Basic GAN Model (MNIST Digits)
# ============================================================

# Import required libraries
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Hyperparameters
# ------------------------------------------------------------
latent_dim = 50
batch_size = 128
epochs = 1000

# ------------------------------------------------------------
# Load and preprocess MNIST dataset
# ------------------------------------------------------------
(X_train, _), _ = tf.keras.datasets.mnist.load_data()
X_train = X_train / 255.0
X_train = X_train.reshape(-1, 28, 28, 1)

# ------------------------------------------------------------
# Generator Model
# ------------------------------------------------------------
generator = tf.keras.Sequential([
    layers.Dense(128, activation='relu', input_dim=latent_dim),
    layers.Dense(256, activation='relu'),
    layers.Dense(784, activation='sigmoid'),
    layers.Reshape((28, 28, 1))
])

# ------------------------------------------------------------
# Discriminator Model
# ------------------------------------------------------------
discriminator = tf.keras.Sequential([
    layers.Flatten(input_shape=(28, 28, 1)),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

discriminator.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# ------------------------------------------------------------
# Build Combined GAN Model
# ------------------------------------------------------------
discriminator.trainable = False

gan_input = layers.Input(shape=(latent_dim,))
fake_image = generator(gan_input)
gan_output = discriminator(fake_image)

gan = tf.keras.Model(gan_input, gan_output)
gan.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),
    loss='binary_crossentropy'
)

# ------------------------------------------------------------
# Function to visualize generated images
# ------------------------------------------------------------
def show_generated_images(epoch):
    noise = np.random.normal(0, 1, (16, latent_dim))
    generated_images = generator.predict(noise, verbose=0)

    plt.figure(figsize=(4,4))
    for i in range(16):
        plt.subplot(4,4,i+1)
        plt.imshow(generated_images[i].reshape(28,28), cmap='gray')
        plt.axis('off')
    plt.suptitle(f"Generated Images at Epoch {epoch}")
    plt.show()

# ------------------------------------------------------------
# Training Loop
# ------------------------------------------------------------
for epoch in range(epochs):

    # ---------------------
    # Train Discriminator
    # ---------------------
    idx = np.random.randint(0, X_train.shape[0], batch_size)
    real_imgs = X_train[idx]

    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    fake_imgs = generator.predict(noise, verbose=0)

    real_labels = np.ones((batch_size, 1))
    fake_labels = np.zeros((batch_size, 1))

    d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)
    d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)

    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

    # ---------------------
    # Train Generator
    # ---------------------
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

    # ---------------------
    # Logging
    # ---------------------
    if epoch % 100 == 0:
        print(f"Epoch {epoch}")
        print(f"Discriminator Loss: {d_loss[0]:.4f}, Accuracy: {d_loss[1]*100:.2f}%")
        print(f"Generator Loss: {g_loss:.4f}")
        show_generated_images(epoch)


# ============================================================
# 9.2 Training a GAN on MNIST Dataset
# ============================================================

# Import required libraries
import tensorflow as tf
from tensorflow.keras import layers
import numpy as np
import matplotlib.pyplot as plt

# ------------------------------------------------------------
# Hyperparameters
# ------------------------------------------------------------
latent_dim = 50
batch_size = 128
epochs = 2000

# ------------------------------------------------------------
# Load and preprocess MNIST dataset
# ------------------------------------------------------------
(X_train, _), _ = tf.keras.datasets.mnist.load_data()
X_train = X_train.astype("float32") / 255.0
X_train = X_train.reshape(-1, 28, 28, 1)

# ------------------------------------------------------------
# Generator Model
# ------------------------------------------------------------
generator = tf.keras.Sequential([
    layers.Dense(128, activation='relu', input_dim=latent_dim),
    layers.Dense(256, activation='relu'),
    layers.Dense(784, activation='sigmoid'),
    layers.Reshape((28, 28, 1))
])

# ------------------------------------------------------------
# Discriminator Model
# ------------------------------------------------------------
discriminator = tf.keras.Sequential([
    layers.Flatten(input_shape=(28, 28, 1)),
    layers.Dense(128, activation='relu'),
    layers.Dense(1, activation='sigmoid')
])

discriminator.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),
    loss='binary_crossentropy'
)

# ------------------------------------------------------------
# Build Combined GAN Model
# ------------------------------------------------------------
discriminator.trainable = False

gan_input = layers.Input(shape=(latent_dim,))
fake_img = generator(gan_input)
gan_output = discriminator(fake_img)

gan = tf.keras.Model(gan_input, gan_output)
gan.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0002),
    loss='binary_crossentropy'
)

# ------------------------------------------------------------
# Function to visualize generated images
# ------------------------------------------------------------
def show_images(epoch):
    noise = np.random.normal(0, 1, (16, latent_dim))
    generated_images = generator.predict(noise, verbose=0)

    plt.figure(figsize=(4, 4))
    for i in range(16):
        plt.subplot(4, 4, i + 1)
        plt.imshow(generated_images[i].reshape(28, 28), cmap='gray')
        plt.axis('off')
    plt.suptitle(f"Generated Images at Epoch {epoch}")
    plt.show()

# ------------------------------------------------------------
# Training Loop
# ------------------------------------------------------------
for epoch in range(epochs):

    # ---------------------
    # Train Discriminator
    # ---------------------
    idx = np.random.randint(0, X_train.shape[0], batch_size)
    real_imgs = X_train[idx]

    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    fake_imgs = generator.predict(noise, verbose=0)

    real_labels = np.ones((batch_size, 1))
    fake_labels = np.zeros((batch_size, 1))

    d_loss_real = discriminator.train_on_batch(real_imgs, real_labels)
    d_loss_fake = discriminator.train_on_batch(fake_imgs, fake_labels)

    d_loss = 0.5 * (d_loss_real + d_loss_fake)

    # ---------------------
    # Train Generator
    # ---------------------
    noise = np.random.normal(0, 1, (batch_size, latent_dim))
    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))

    # ---------------------
    # Logging
    # ---------------------
    if epoch % 200 == 0:
        print(f"Epoch {epoch}")
        print(f"Discriminator Loss: {d_loss:.4f}")
        print(f"Generator Loss: {g_loss:.4f}")
        show_images(epoch)


# ============================================================
# 9.3 Qualitative Evaluation of GAN Image Quality
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import pairwise_distances
import tensorflow as tf
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import load_model

# ------------------------------------------------------------
# Load real MNIST dataset
# ------------------------------------------------------------
(X_train, _), _ = mnist.load_data()
X_train = X_train.astype("float32") / 255.0
X_train = X_train.reshape(-1, 28, 28, 1)

# ------------------------------------------------------------
# Load trained generator (if not already in memory)
# ------------------------------------------------------------
# Uncomment if generator was saved earlier
# generator = load_model("trained_generator.h5")

# If generator already exists in memory from training, skip loading
# ------------------------------------------------------------

latent_dim = 50

# ============================================================
# Utility: Plot image grid
# ============================================================
def plot_grid(imgs, rows=8, cols=8, title=None):
    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows))
    imgs = imgs[:rows * cols]
    for i, ax in enumerate(axes.flat):
        ax.imshow(imgs[i].squeeze(), cmap='gray')
        ax.axis('off')
    if title:
        plt.suptitle(title)
    plt.tight_layout()
    plt.show()

# ============================================================
# Function 1: Generate & visualize samples (Sample Grid)
# ============================================================
def generate_images(num_samples=64):
    noise = np.random.normal(0, 1, (num_samples, latent_dim))
    imgs = generator.predict(noise, verbose=0)
    return imgs

generated_imgs = generate_images(64)
print("Showing generated image grid...")
plot_grid(generated_imgs, title="Generated Samples")

# ============================================================
# Function 2: Latent Space Interpolation
# ============================================================
def interpolate(z1, z2, steps=10):
    alphas = np.linspace(0, 1, steps)
    return np.array([(1 - a) * z1 + a * z2 for a in alphas])

print("Showing latent interpolation...")
z1 = np.random.normal(size=(latent_dim,))
z2 = np.random.normal(size=(latent_dim,))
z_interp = interpolate(z1, z2, steps=10)

imgs_interp = generator.predict(z_interp, verbose=0)
plot_grid(imgs_interp, rows=1, cols=10, title="Latent Interpolation")

# ============================================================
# Function 3: Nearest Neighbor Comparison (Memorization Check)
# ============================================================
print("Computing nearest neighbor comparisons...")

gen_flat = generated_imgs.reshape(len(generated_imgs), -1)
real_flat = X_train.reshape(len(X_train), -1)

# Compute distances
dists = pairwise_distances(gen_flat, real_flat, metric='euclidean')
nn_idx = dists.argmin(axis=1)

print("Showing generated images vs nearest real images...")
plt.figure(figsize=(12, 4))

for i in range(10):
    # Generated image
    plt.subplot(2, 10, i + 1)
    plt.imshow(generated_imgs[i].squeeze(), cmap='gray')
    plt.axis('off')

    # Nearest real image
    plt.subplot(2, 10, 10 + i + 1)
    plt.imshow(X_train[nn_idx[i]].squeeze(), cmap='gray')
    plt.axis('off')

plt.suptitle("Top: Generated | Bottom: Nearest Real MNIST")
plt.show()

# ============================================================
# Function 4: Diversity / Mode Collapse Check
# ============================================================
print("Performing diversity check...")

variances = gen_flat.var(axis=0)
print("Average pixel variance:", variances.mean())

# Visualize variance distribution
plt.figure(figsize=(6, 4))
plt.hist(variances, bins=50)
plt.title("Pixel Variance Distribution (Diversity Check)")
plt.xlabel("Variance")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

